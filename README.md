# ðŸ§  Attention-Based Date Format Translator

This repository implements a custom sequence-to-sequence model with an attention mechanism to **translate natural language date formats** (e.g., `"21st of August 2016"`) into machine-readable formats (e.g., `"2016-08-21"`). The model is built from scratch using **TensorFlow/Keras**, and includes a **custom attention layer**, a **Bi-LSTM encoder**, and an **LSTM decoder**.

---

## Project Structure
```
attention-date-translator/
â”‚
â”œâ”€â”€ notebook.ipynb # Main Jupyter notebook (training + inference)
â”œâ”€â”€ models/
â”‚ â””â”€â”€ model.h5 # Saved trained model
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ dataset.pkl # Serialized dataset
â”‚ â”œâ”€â”€ human_vocab.json # Input vocabulary (character-level)
â”‚ â”œâ”€â”€ machine_vocab.json # Output vocabulary
â”‚ â””â”€â”€ inv_machine_vocab.json # Inverse output vocab (for decoding)
â”œâ”€â”€ images/
â”‚ â”œâ”€â”€ attn_mechanism.png # Attention mechanism diagram
â”‚ â””â”€â”€ attn_model.png 
â””â”€â”€ README.md
```

---

## Model Overview

- **Input**: `"3rd of March 2023"`
- **Output**: `"2023-03-03"`
- **Architecture**:
  - Bi-LSTM encoder
  - Custom-built attention mechanism
  - LSTM decoder
  - Dense output layer with softmax
- **Training**: Character-level, multi-class classification at each output time step

---

## ðŸ§  How Attention Works

<p align="center">
  <img src="images/attn_mechanism.png" width="500"/>
</p>

Each decoder step **attends** over all encoder hidden states to produce a context vector that helps predict the correct output character.

---

## Sample Predictions

```python
Input:    "21st of August 2016"
Predicted Output: "2016-08-21"

Input:    "Tuesday May 5 2009"
Predicted Output: "2009-05-05"

Input:    "1 March 2001"
Predicted Output: "2001-03-01"
```
---

## Dataset
Generated synthetically using the faker and babel libraries.

Human-readable dates are randomly created in various formats.

Output format is consistent: YYYY-MM-DD.

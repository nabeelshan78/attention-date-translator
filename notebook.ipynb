{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating Human Readable Dates Into Machine Readable Dates\n",
    "\n",
    "* The model we will build here could be used to translate from one language to another, such as translating from English to Hindi. \n",
    "* However, language translation requires massive datasets and usually takes days of training on GPUs. \n",
    "* So, we will perform a simpler \"date translation\" task. \n",
    "* The network will input a date written in a variety of possible formats (*e.g. \"the 29th of August 1958\", \"03/30/1968\", \"24 JUNE 1987\"*) \n",
    "* The network will translate them into standardized, machine readable dates (*e.g. \"1958-08-29\", \"1968-03-30\", \"1987-06-24\"*). \n",
    "* We will have the network learn to output dates in the common machine-readable format YYYY-MM-DD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from faker import Faker\n",
    "from babel.dates import format_date\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, LSTM, Dense, Activation, RepeatVector, Bidirectional,\n",
    "    Concatenate, Dot, Permute, Multiply, Lambda\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Faker.seed(12345)\n",
    "random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMATS = ['short',\n",
    "           'medium',\n",
    "           'long',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'd MMM YYY', \n",
    "           'd MMMM YYY',\n",
    "           'dd MMM YYY',\n",
    "           'd MMM, YYY',\n",
    "           'd MMMM, YYY',\n",
    "           'dd, MMM YYY',\n",
    "           'd MM YY',\n",
    "           'd MMMM YYY',\n",
    "           'MMMM d YYY',\n",
    "           'MMMM d, YYY',\n",
    "           'dd.MM.YY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_date():\n",
    "    \"\"\"\n",
    "        Loads some fake dates \n",
    "        :returns: tuple containing human readable string, machine readable string, and date object\n",
    "    \"\"\"\n",
    "    fake = Faker()\n",
    "    dt = fake.date_object()\n",
    "\n",
    "    try:\n",
    "        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='en_US') # locale=random.choice(LOCALES))\n",
    "        human_readable = human_readable.lower()\n",
    "        human_readable = human_readable.replace(',','')\n",
    "        machine_readable = dt.isoformat()\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        return None, None, None\n",
    "\n",
    "    return human_readable, machine_readable, dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(m):\n",
    "    \"\"\"\n",
    "        Loads a dataset with m examples and vocabularies\n",
    "        :m: the number of examples to generate\n",
    "    \"\"\"\n",
    "    \n",
    "    human_vocab = set()\n",
    "    machine_vocab = set()\n",
    "    dataset = []\n",
    "    Tx = 30\n",
    "    \n",
    "\n",
    "    for i in tqdm(range(m)):\n",
    "        h, m, _ = load_date()\n",
    "        if h is not None:\n",
    "            dataset.append((h, m))\n",
    "            human_vocab.update(tuple(h))\n",
    "            machine_vocab.update(tuple(m))\n",
    "    \n",
    "    human = dict(zip(sorted(human_vocab) + ['<unk>', '<pad>'], \n",
    "                     list(range(len(human_vocab) + 2))))\n",
    "    inv_machine = dict(enumerate(sorted(machine_vocab)))\n",
    "    machine = {v:k for k,v in inv_machine.items()}\n",
    " \n",
    "    return dataset, human, machine, inv_machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [04:02<00:00, 41.18it/s]\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "with open(\"data/dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dataset, f)\n",
    "\n",
    "with open(\"data/human_vocab.json\", \"w\") as f:\n",
    "    json.dump(human_vocab, f)\n",
    "\n",
    "with open(\"data/machine_vocab.json\", \"w\") as f:\n",
    "    json.dump(machine_vocab, f)\n",
    "\n",
    "with open(\"data/inv_machine_vocab.json\", \"w\") as f:\n",
    "    json.dump(inv_machine_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "with open(\"data/dataset.pkl\", \"rb\") as f:\n",
    "    loaded_dataset = pickle.load(f)\n",
    "\n",
    "# Load vocabularies\n",
    "with open(\"data/human_vocab.json\", \"r\") as f:\n",
    "    loaded_human_vocab = json.load(f)\n",
    "\n",
    "with open(\"data/machine_vocab.json\", \"r\") as f:\n",
    "    loaded_machine_vocab = json.load(f)\n",
    "\n",
    "with open(\"data/inv_machine_vocab.json\", \"r\") as f:\n",
    "    loaded_inv_machine_vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = loaded_dataset\n",
    "human_vocab = loaded_human_vocab\n",
    "machine_vocab = loaded_machine_vocab\n",
    "inv_machine_vocab = loaded_inv_machine_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples loaded: 10000\n",
      "\n",
      "Sample data pairs:\n",
      "  1. Human: 9 may 1998  ->  Machine: 1998-05-09\n",
      "  2. Human: 10.11.19  ->  Machine: 2019-11-10\n",
      "  3. Human: 9/10/70  ->  Machine: 1970-09-10\n",
      "  4. Human: monday august 19 2024  ->  Machine: 2024-08-19\n",
      "  5. Human: saturday april 28 1990  ->  Machine: 1990-04-28\n",
      "\n",
      "Input vocabulary (human_vocab):\n",
      "  Size: 37\n",
      "  Example mapping: [(' ', 0), ('.', 1), ('/', 2), ('0', 3), ('1', 4)]\n",
      "\n",
      "Output vocabulary (machine_vocab):\n",
      "  Size: 11\n",
      "  Example mapping: [('-', 0), ('0', 1), ('1', 2), ('2', 3), ('3', 4)]\n",
      "\n",
      "Inverse machine vocab (inv_machine_vocab):\n",
      "  Size: 11\n",
      "  Example mapping: [('0', '-'), ('1', '0'), ('2', '1'), ('3', '2'), ('4', '3')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Total examples loaded:\", len(dataset))\n",
    "\n",
    "print(\"\\nSample data pairs:\")\n",
    "for i in range(5):\n",
    "    human_str, machine_str = dataset[i]\n",
    "    print(f\"  {i+1}. Human: {human_str}  ->  Machine: {machine_str}\")\n",
    "\n",
    "print(\"\\nInput vocabulary (human_vocab):\")\n",
    "print(f\"  Size: {len(human_vocab)}\")\n",
    "print(f\"  Example mapping: {list(human_vocab.items())[:5]}\")\n",
    "\n",
    "print(\"\\nOutput vocabulary (machine_vocab):\")\n",
    "print(f\"  Size: {len(machine_vocab)}\")\n",
    "print(f\"  Example mapping: {list(machine_vocab.items())[:5]}\")\n",
    "\n",
    "print(\"\\nInverse machine vocab (inv_machine_vocab):\")\n",
    "print(f\"  Size: {len(inv_machine_vocab)}\")\n",
    "print(f\"  Example mapping: {list(inv_machine_vocab.items())[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_int(string, length, vocab):\n",
    "    \"\"\"\n",
    "    Converts a string to a fixed-length list of integers based on the provided vocabulary.\n",
    "    \n",
    "    Arguments:\n",
    "    string -- Input string (e.g., 'Wed 10 Jul 2007')\n",
    "    length -- Desired length of output sequence (pads or cuts accordingly)\n",
    "    vocab -- Dictionary mapping characters to integer indices\n",
    "    \n",
    "    Returns:\n",
    "    rep -- List of integers of size 'length', representing the string\n",
    "    \"\"\"\n",
    "    \n",
    "    string = string.lower().replace(',', '')\n",
    "    \n",
    "    if len(string) > length:\n",
    "        string = string[:length]\n",
    "    \n",
    "    rep = [vocab.get(char, '<unk>') for char in string]\n",
    "    \n",
    "    if len(string) < length:\n",
    "        rep += [vocab['<pad>']] * (length - len(string))\n",
    "    \n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
    "    \"\"\"\n",
    "    Prepares input and output sequences for training a sequence-to-sequence model.\n",
    "    \n",
    "    Arguments:\n",
    "    dataset       -- List of (human_readable, machine_readable) string pairs\n",
    "    human_vocab   -- Dictionary mapping input characters to integer indices\n",
    "    machine_vocab -- Dictionary mapping output characters to integer indices\n",
    "    Tx            -- Length of input sequences (fixed)\n",
    "    Ty            -- Length of output sequences (fixed)\n",
    "    \n",
    "    Returns:\n",
    "    X     -- np.array of shape (m, Tx), input sequences (integers)\n",
    "    Y     -- np.array of shape (m, Ty), output sequences (integers)\n",
    "    Xoh   -- np.array of shape (m, Tx, len(human_vocab)), one-hot encoded inputs\n",
    "    Yoh   -- np.array of shape (m, Ty, len(machine_vocab)), one-hot encoded outputs\n",
    "    \"\"\"\n",
    "\n",
    "    X_texts, Y_texts = zip(*dataset)\n",
    "    \n",
    "    X = np.array([string_to_int(x, Tx, human_vocab) for x in X_texts])\n",
    "    Y = np.array([string_to_int(y, Ty, machine_vocab) for y in Y_texts])\n",
    "    \n",
    "    Xoh = np.array([to_categorical(x_seq, num_classes=len(human_vocab)) for x_seq in X])\n",
    "    Yoh = np.array([to_categorical(y_seq, num_classes=len(machine_vocab)) for y_seq in Y])\n",
    "    \n",
    "    return X, Y, Xoh, Yoh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: (10000, 30, 37)\n",
      "Yoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "Tx = 30\n",
    "Ty = 10  # YYYY-MM-DD is 10 characters long.\n",
    "\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: 9 may 1998\n",
      "Target date: 1998-05-09\n",
      "\n",
      "Source after preprocessing (indices): [12  0 24 13 34  0  4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
      " 36 36 36 36 36 36]\n",
      "Target after preprocessing (indices): [ 2 10 10  9  0  1  6  0  1 10]\n",
      "\n",
      "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Neural Machine Translation with Attention\n",
    "\n",
    "* If we had to translate a book's paragraph from French to English, we would not read the whole paragraph, then close the book and translate. \n",
    "* Even during the translation process, we would read/re-read and focus on the parts of the French paragraph corresponding to the parts of the English we are writing down. \n",
    "* The attention mechanism tells a Neural Machine Translation model where it should pay attention to at any step. \n",
    "\n",
    "### 2.1 - Attention Mechanism\n",
    "\n",
    "In this part, we will implement the attention mechanism.  \n",
    "\n",
    "\n",
    "* The diagram on the left shows the attention model. \n",
    "* The diagram on the right shows what one \"attention\" step does to calculate the attention variables $\\alpha^{\\langle t, t' \\rangle}$.\n",
    "* The attention variables $\\alpha^{\\langle t, t' \\rangle}$ are used to compute the context variable $context^{\\langle t \\rangle}$ for each timestep in the output ($t=1, \\ldots, T_y$). \n",
    "\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"images/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"images/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "</table>\n",
    "<caption><center>Neural machine translation with attention</center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-attention and Post-attention LSTMs on both sides of the attention mechanism\n",
    "- There are two separate LSTMs in this model (see diagram on the left): pre-attention and post-attention LSTMs.\n",
    "- *Pre-attention* Bi-LSTM is the one at the bottom of the picture is a Bi-directional LSTM and comes *before* the attention mechanism.\n",
    "    - The attention mechanism is shown in the middle of the left-hand diagram.\n",
    "    - The pre-attention Bi-LSTM goes through $T_x$ time steps\n",
    "- *Post-attention* LSTM: at the top of the diagram comes *after* the attention mechanism. \n",
    "    - The post-attention LSTM goes through $T_y$ time steps. \n",
    "\n",
    "- The post-attention LSTM passes the hidden state $s^{\\langle t \\rangle}$ and cell state $c^{\\langle t \\rangle}$ from one time step to the next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each time step does not use predictions from the previous time step\n",
    "* The post-attention LSTM at time $t$ does not take the previous time step's prediction $y^{\\langle t-1 \\rangle}$ as input.\n",
    "* The post-attention LSTM at time 't' only takes the hidden state $s^{\\langle t\\rangle}$ and cell state $c^{\\langle t\\rangle}$ as input. \n",
    "* Because unlike language generation (where adjacent characters are highly correlated) there isn't as strong a dependency between the previous character and the next character in a YYYY-MM-DD date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=1):\n",
    "    \"\"\"Softmax activation function.\n",
    "    # Arguments\n",
    "        x : Tensor.\n",
    "        axis: Integer, axis along which the softmax normalization is applied.\n",
    "    # Returns\n",
    "        Tensor, output of softmax transformation.\n",
    "    # Raises\n",
    "        ValueError: In case `dim(x) == 1`.\n",
    "    \"\"\"\n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 2:\n",
    "        return K.softmax(x)\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Computes the context vector using attention mechanism for one decoder time step.\n",
    "\n",
    "    Arguments:\n",
    "    a -- Encoder hidden states, shape (m, Tx, 2*n_a)\n",
    "    s_prev -- Previous decoder hidden state, shape (m, n_s)\n",
    "\n",
    "    Returns:\n",
    "    context -- Context vector, shape (m, 1, 2*n_a)\n",
    "    \"\"\"\n",
    "    s_prev = repeator(s_prev)                      # (m, Tx, n_s)\n",
    "    concat = concatenator([a, s_prev])             # (m, Tx, 2*n_a + n_s)\n",
    "    e = densor1(concat)                            # (m, Tx, 10)\n",
    "    energies = densor2(e)                          # (m, Tx, 1)\n",
    "    alphas = activator(energies)                   # (m, Tx, 1)\n",
    "    context = dotor([alphas, a])                   # (m, 1, 2*n_a)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 32 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n",
    "n_s = 64 # number of units for the post-attention LSTM's hidden state \"s\"\n",
    "\n",
    "# this is the post attention LSTM cell.  \n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelf(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size=None):\n",
    "    \"\"\"\n",
    "    Builds a sequence-to-sequence model with attention.\n",
    "\n",
    "    Arguments:\n",
    "    Tx -- input sequence length\n",
    "    Ty -- output sequence length\n",
    "    n_a -- Bi-LSTM hidden units\n",
    "    n_s -- post-attention LSTM hidden units\n",
    "    human_vocab_size -- size of input vocabulary\n",
    "    machine_vocab_size -- (optional) size of output vocabulary\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s, c = s0, c0\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
    "\n",
    "    for t in range(Ty):\n",
    "        context = one_step_attention(a, s)\n",
    "        _, s, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
    "        out = output_layer(s)\n",
    "        outputs.append(out)\n",
    "\n",
    "    model = Model(inputs=[X, s0, c0], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelf(Tx, Ty, n_a, n_s, len(human_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 30, 37)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 30, 64)       17920       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[1][1]                     \n",
      "                                                                 lstm_2[2][1]                     \n",
      "                                                                 lstm_2[3][1]                     \n",
      "                                                                 lstm_2[4][1]                     \n",
      "                                                                 lstm_2[5][1]                     \n",
      "                                                                 lstm_2[6][1]                     \n",
      "                                                                 lstm_2[7][1]                     \n",
      "                                                                 lstm_2[8][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30, 128)      0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[3][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[4][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[5][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[6][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[7][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[8][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[9][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 30, 10)       1290        concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 concatenate_1[2][0]              \n",
      "                                                                 concatenate_1[3][0]              \n",
      "                                                                 concatenate_1[4][0]              \n",
      "                                                                 concatenate_1[5][0]              \n",
      "                                                                 concatenate_1[6][0]              \n",
      "                                                                 concatenate_1[7][0]              \n",
      "                                                                 concatenate_1[8][0]              \n",
      "                                                                 concatenate_1[9][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 30, 1)        11          dense_3[0][0]                    \n",
      "                                                                 dense_3[1][0]                    \n",
      "                                                                 dense_3[2][0]                    \n",
      "                                                                 dense_3[3][0]                    \n",
      "                                                                 dense_3[4][0]                    \n",
      "                                                                 dense_3[5][0]                    \n",
      "                                                                 dense_3[6][0]                    \n",
      "                                                                 dense_3[7][0]                    \n",
      "                                                                 dense_3[8][0]                    \n",
      "                                                                 dense_3[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 30, 1)        0           dense_4[0][0]                    \n",
      "                                                                 dense_4[1][0]                    \n",
      "                                                                 dense_4[2][0]                    \n",
      "                                                                 dense_4[3][0]                    \n",
      "                                                                 dense_4[4][0]                    \n",
      "                                                                 dense_4[5][0]                    \n",
      "                                                                 dense_4[6][0]                    \n",
      "                                                                 dense_4[7][0]                    \n",
      "                                                                 dense_4[8][0]                    \n",
      "                                                                 dense_4[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 64), (None,  33024       dot_1[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_1[1][0]                      \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "                                                                 dot_1[2][0]                      \n",
      "                                                                 lstm_2[1][1]                     \n",
      "                                                                 lstm_2[1][2]                     \n",
      "                                                                 dot_1[3][0]                      \n",
      "                                                                 lstm_2[2][1]                     \n",
      "                                                                 lstm_2[2][2]                     \n",
      "                                                                 dot_1[4][0]                      \n",
      "                                                                 lstm_2[3][1]                     \n",
      "                                                                 lstm_2[3][2]                     \n",
      "                                                                 dot_1[5][0]                      \n",
      "                                                                 lstm_2[4][1]                     \n",
      "                                                                 lstm_2[4][2]                     \n",
      "                                                                 dot_1[6][0]                      \n",
      "                                                                 lstm_2[5][1]                     \n",
      "                                                                 lstm_2[5][2]                     \n",
      "                                                                 dot_1[7][0]                      \n",
      "                                                                 lstm_2[6][1]                     \n",
      "                                                                 lstm_2[6][2]                     \n",
      "                                                                 dot_1[8][0]                      \n",
      "                                                                 lstm_2[7][1]                     \n",
      "                                                                 lstm_2[7][2]                     \n",
      "                                                                 dot_1[9][0]                      \n",
      "                                                                 lstm_2[8][1]                     \n",
      "                                                                 lstm_2[8][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 11)           715         lstm_2[0][1]                     \n",
      "                                                                 lstm_2[1][1]                     \n",
      "                                                                 lstm_2[2][1]                     \n",
      "                                                                 lstm_2[3][1]                     \n",
      "                                                                 lstm_2[4][1]                     \n",
      "                                                                 lstm_2[5][1]                     \n",
      "                                                                 lstm_2[6][1]                     \n",
      "                                                                 lstm_2[7][1]                     \n",
      "                                                                 lstm_2[8][1]                     \n",
      "                                                                 lstm_2[9][1]                     \n",
      "==================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01) \n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10000, 11)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Yoh.swapaxes(0, 1)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10000\n",
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 9s 89ms/step - loss: 16.3766 - dense_5_loss: 1.0274 - dense_5_1_loss: 0.9519 - dense_5_2_loss: 1.7549 - dense_5_3_loss: 2.6668 - dense_5_4_loss: 0.7499 - dense_5_5_loss: 1.2944 - dense_5_6_loss: 2.6597 - dense_5_7_loss: 0.9748 - dense_5_8_loss: 1.7133 - dense_5_9_loss: 2.5835 - dense_5_accuracy: 0.6140 - dense_5_1_accuracy: 0.7583 - dense_5_2_accuracy: 0.2914 - dense_5_3_accuracy: 0.0787 - dense_5_4_accuracy: 0.9085 - dense_5_5_accuracy: 0.3283 - dense_5_6_accuracy: 0.0621 - dense_5_7_accuracy: 0.9114 - dense_5_8_accuracy: 0.2337 - dense_5_9_accuracy: 0.0975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x739d6a498ed0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('models/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the results on new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 3 May 1979\n",
      "output: 1979-05-33 \n",
      "\n",
      "source: 5 April 09\n",
      "output: 2009-04-05 \n",
      "\n",
      "source: 21th of August 2016\n",
      "output: 2016-08-20 \n",
      "\n",
      "source: Tue 10 Jul 2007\n",
      "output: 2007-07-10 \n",
      "\n",
      "source: Saturday May 9 2018\n",
      "output: 2018-05-09 \n",
      "\n",
      "source: March 3 2001\n",
      "output: 2001-03-03 \n",
      "\n",
      "source: March 3rd 2001\n",
      "output: 2001-03-03 \n",
      "\n",
      "source: 1 March 2001\n",
      "output: 2001-03-01 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', \n",
    "            'Tue 10 Jul 2007', 'Saturday May 9 2018', \n",
    "            'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "\n",
    "s00 = np.zeros((1, n_s))\n",
    "c00 = np.zeros((1, n_s))\n",
    "\n",
    "for example in EXAMPLES:\n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    source = np.array([to_categorical(x, num_classes=len(human_vocab)) for x in source])\n",
    "    source = np.expand_dims(source, axis=0)\n",
    "\n",
    "    prediction = model.predict([source, s00, c00])\n",
    "    prediction = np.argmax(prediction, axis=-1)\n",
    "\n",
    "    output = [inv_machine_vocab[str(i)] for i in prediction.flatten()]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
